% arara: xelatex
\documentclass[12pt]{article}

% \usepackage{physics}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\usepackage{tikzducks}

\usepackage{tikz} % картинки в tikz
\usetikzlibrary{shapes, arrows, positioning}
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering}

\usepackage{amsmath, amsfonts, amssymb} % куча стандартных математических плюшек

\usepackage{comment}

\usepackage[top=2cm, left=1.2cm, right=1.2cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption}

\usepackage{url} % to use \url{link to web}


\newcommand{\smallduck}{\begin{tikzpicture}[scale=0.3]
    \duck[
        cape=black,
        hat=black,
        mask=black
    ]
    \end{tikzpicture}}

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{Econometrics for pandas}
\chead{}
\rhead{December exam}
\lfoot{}
\cfoot{}
\rfoot{}


\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\usepackage{tcolorbox} % рамочки!

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos - печатает все поставленные \todo'шки


% более красивые таблицы
\usepackage{booktabs}
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"


\setcounter{MaxMatrixCols}{20}
% by crazy default pmatrix supports only 10 cols :)


\usepackage{fontspec}
\usepackage{libertine}
\usepackage{polyglossia}

\setmainlanguage{russian}
\setotherlanguages{english}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
% \setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
% \newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
% \setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*}

%% эконометрические сокращения
\DeclareMathOperator{\Cov}{\mathbb{C}ov}
\DeclareMathOperator{\Corr}{\mathbb{C}orr}
\DeclareMathOperator{\Var}{\mathbb{V}ar}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\loss}{loss}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\plim}{plim}


\DeclareMathOperator{\rank}{rank}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}

\DeclareMathOperator{\E}{\mathbb{E}}
% \DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\card}{card}

\DeclareMathOperator{\Convex}{Convex}


\newcommand{\cN}{\mathcal{N}}
\newcommand{\cF}{\mathcal{F}}


\newcommand{\SST}{\text{SST}}
\newcommand{\SSR}{\text{SS}^{\text{res}}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\hb}{\hat{\beta}}
\newcommand{\dPois}{\mathrm{Pois}}

\usepackage{mathtools}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\scalp}{\langle}{\rangle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}



\begin{document}

\begin{enumerate}
    \item {[10]} I have two almost identical regressors, $b = 100a$, 
    one is measured in dollars and the other one — in cents. 
    Consider the ridge regression loss function,  
    \[
    \loss(\hb_a, \hb_b) = \sum_i (y_i - \hat y_i)^2 + \lambda \cdot (\hb_a^2 + \hb_b^2), \quad \hat y_i = \hb_a a_i + \hb_b b_i.
    \] 
    \begin{enumerate}
        \item {[7]} Find the penalized estimates $\hb_a$ and $\hb_b$ for arbitrary $\lambda > 0$.
    \end{enumerate}
    In the regression $\hat y_i = \hat\gamma_1 a_i$ the OLS estimator $\hat\gamma_1$ is equal to $2$.
    \begin{enumerate}[resume]
        \item {[3]} What are the approximate values of $\hb_a$ and $\hb_b$ in the penalized model for small $\lambda \approx 0$?
    \end{enumerate}
    
    \item {[10]} Elon Musk studies the model $y_i = \beta_0 + \beta_1 a_i + \beta_2 b_i + u_i$.
    He has $500$ observations and he knows the heteroskedasticity form up to an unknown constant.
    To obtain the most efficient unbiased estimators $\hb$ he has divided each observation by $a_i b_i$. 

    \begin{enumerate}
        \item {[5]} What is the variance $\Var(u_i \mid X)$ supposed by Elon Musk?
        \item {[5]} To check whether the heteroskedasticity is really present Elon Musk estimated the regression 
        \[
        \hat v_i = \hat \gamma_0 + \hat\gamma_1 a_i + \hat\gamma_2 b_i + \hat\gamma_3 a_i b_i, \quad R^2 = 0.05,
        \]
        where $v_i$ are the squared residuals $\hat u_i^2$ from original model. 

       What is the statistical conclusion about the heteroskedasticity presence?
    \end{enumerate}

    Critical values for 5\% significance level: $\chi^2_1 = 3.84$, $\chi^2_2= 5.99$, $\chi^2_3=7.81$, $\chi^2_{498} = 549$, $\chi^2_{499} = 551$, $\chi^2_{500} = 553$.


    \item {[10]} Donald Trump has $403$ observations. 
    He estimated the first simple regression:
    \[
    \hat x_i = 2 - 2 w_i, \quad R^2 = 0.81, \quad \SST = 100 \quad (\text{Regression A}).
    \]
    Than he estimated the second regression,
    \[
    \hat w_i = \hat \gamma_0 + \hat\gamma_1 x_i, \quad \SST = 200 \quad (\text{Regression B}).
    \]
    \begin{enumerate}
        \item {[2]} Find $R^2$ in the regression $B$.
        % \item {[2]} Find $\hat\gamma_1$ in the regression $B$.
    \end{enumerate}
    Than he estimated the third regression, 
    \[
    \hat y_i = 2 + 3x_i + 5w_i, \quad R^2 = 0.6, \quad \SST = 500 \quad (\text{Regression C}).
    \]
    \begin{enumerate}[resume]
        \item {[3]} Find the variance inflation factors of $x_i$ and $w_i$ in the regression $C$.
        \item {[5]} Find the 95\% confidence interval for $\partial y/\partial w$.
    \end{enumerate}
    \newpage 


    \item  {[10]} All regressors in $X$ matrix are standardized. 
    There are $3$ regressors and $10000$ observations. 
    You partially know the singular value decomposition of $X$:
    \[
    X = UDV^T, \diag(D) = (5 \cdot 10^5, 10^5, 10^4),
    V = \begin{pmatrix}
        0.000156 & 0.005953 & -0.999982 \\ 
        0.007737 & 0.999952 & 0.005954 \\ 
        0.999970 & -0.007737 & 0.000109 \\ 
        \end{pmatrix}.
    \]
    \begin{enumerate}
        \item {[3]} Find the sample variance of each principal component.
        \item {[3]} How much variance in \% does the first principal component explain?
        \item {[4]} Express the first original regressor as the function of the three principal components.
    \end{enumerate}


    % \item {[10]} Consider the matrix $X = \begin{pmatrix}
    %     1 & 1 \\
    %     1 & 1 \\
    % \end{pmatrix}$.
    
    % \begin{enumerate}
    %     \item {[4]} Find the matrix $X^TX$ and diagonalize it. 
    %     \item {[3]} Find the SVD of $X$. 
    % \end{enumerate}
    
    % To find the generalized Moore-Penrose inverse of the diagonal matrix $D$ 
    % one should keep all zero elements and replace all non-zero elements $d_{ii}$ by $1/d_{ii}$.
    
    % The Moore-Penrose inverse of an arbitrary matrix $M$ with SVD $M = U D V^T$ is defined as
    % $M^+ = V D^+ U^T$ where $D^+$ is the Moore-Penrose inverse of the diagonal matrix $D$.
    % \begin{enumerate}[resume]
    %     \item {[3]} Find the Moore-Penrose inverse $X^+$.
    % \end{enumerate}

    \item {[10]} Consider the model $y_i = \beta_0 + \beta_1 x_i + u_i$ with independent and identically distributed observations.
    The regressor $x_i$ is correlated with $u_i$ and with variable $z_i$. 
    The variable $z_i$ is uncorrelated with $u_i$. 

    The variable $z_i$ is binary and takes only values $0$ or $1$. 
    
    The regressor $x_i$ is continuous and all observations may be divided into two groups:
        
        \begin{tabular}{lccc}
            \toprule
            Group & $\bar y_z$  & $\bar x_z$ & $n_z$ \\
            \midrule
            $z = 0$ & $5$  & $4$  & $500$ \\
            $z = 1$ & $7$  & $2$  & $800$ \\
          \bottomrule
        \end{tabular}

    Here $\bar y_z$, $\bar x_z$ and $n_z$ are average values of $y_i$, $x_i$ and the number of observations for each group.

    \begin{enumerate}
        \item {[8]} Find the instrumental variable estimate $\hb_1$.
        \item {[2]} Will the estimate change if the labels $z = 0$ and $z = 1$ will be switched?
    \end{enumerate}
    

    
        % The regressor $x_i$ is binary and all observations may be divided into four groups:
        
        % \begin{tabular}{lcc}
        %     \toprule
        %      & $x = 0$  & $x = 1$ \\
        %      \midrule
        %     $z = 0$ & $\bar y_{00} = 20$, $n_{00} =  300$ & $\bar y_{01} = 20$, $n_{01} =  500$  \\
        %     $z = 1$ & $\bar y_{10} = 15$, $n_{10} =  400$  & $\bar y_{11} = 35$, $n_{11} =  200$  \\
        %   \bottomrule
        % \end{tabular}

        % Here $\bar y_{zx}$ and $n_{zx}$ are average values of $y_i$ and the number of observations for each group.



    \item (based on past LSE exams) Consider the simple linear regression model:
    \[
    y_i = \beta_0 + \beta_1 x_i + u_i, \quad i = 1, \dots, n.
    \]
    We assume that the errors $\{u_i\}_{i=1}^n$ are independent random variables with zero mean. 
    
    The regressor $\{x_i\}_{i=1}^n$ is stochastic and Gauss-Markov conditions are satisfied.
    Under these conditions, the OLS estimator for $\beta_1$, $\hb_1$, is conditionally unbiased. 
    You are not asked to derive $\hb_1$.
    
    \begin{enumerate}
        \item {[2]} Explain the concept of unbiasedness of an estimator. 
    
        \item {[5]} Let us consider two other estimators for the slope \(\beta_1\):
        \[
        \hat{\beta}_1^\circ = \frac{\sum_{i=1}^n (z_i - \bar{z}) y_i}{\sum_{i=1}^n (z_i - \bar{z}) x_i} \quad \text{and} \quad
        \hat{\beta}_1^* = \frac{\sum_{i=1}^n (z_i - \bar{z}) y_i}{\sum_{i=1}^n (z_i - \bar{z}) z_i}
        \]
        where \(z_i = \sqrt{x_i}\) for all \(i\) and \(\bar{z} = \frac{1}{n} \sum_{i=1}^n z_i\). 
        Please indicate whether \(\hat{\beta}_1^\circ\) and \(\hat{\beta}_1^*\) are conditionally unbiased estimators for \(\beta_1\). 
        Clearly show your derivations. 
    
        \item {[3]} Briefly indicate how you would choose between the three estimators, \(\hat{\beta}_1\), \(\hat{\beta}_1^\circ\), and \(\hat{\beta}_1^*\). 
    \end{enumerate}

\end{enumerate}


\end{document}

