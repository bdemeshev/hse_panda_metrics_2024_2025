% arara: xelatex
\documentclass[12pt]{article}

% Econometrics, 2024-2025

% \usepackage{physics}


\usepackage{tikzducks}

\usepackage{tikz} % картинки в tikz
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering}

\usepackage{amsmath, amsfonts, amssymb} % куча стандартных математических плюшек

\usepackage{comment}

\usepackage[top=2cm, left=1.2cm, right=1.2cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption}

\usepackage{url} % to use \url{link to web}


\newcommand{\smallduck}{\begin{tikzpicture}[scale=0.3]
    \duck[
        cape=black,
        hat=black,
        mask=black
    ]
    \end{tikzpicture}}

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{Home assignments for samurai}
\lfoot{}
\cfoot{}
\rfoot{}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\usepackage{tcolorbox} % рамочки!

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos - печатает все поставленные \todo'шки


% более красивые таблицы
\usepackage{booktabs}
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"


\setcounter{MaxMatrixCols}{20}
% by crazy default pmatrix supports only 10 cols :)


\usepackage{fontspec}
\usepackage{libertine}
\usepackage{polyglossia}

\setmainlanguage{russian}
\setotherlanguages{english}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
% \setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
% \newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
% \setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*}

%% эконометрические сокращения
\DeclareMathOperator{\Cov}{\mathbb{C}ov}
\DeclareMathOperator{\Corr}{\mathbb{C}orr}
\DeclareMathOperator{\Var}{\mathbb{V}ar}
\DeclareMathOperator{\hVar}{\widehat{\mathbb{V}ar}}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\row}{row}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}

\DeclareMathOperator{\E}{\mathbb{E}}
% \DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\card}{card}

\DeclareMathOperator{\Convex}{Convex}

\newcommand \cN{\mathcal{N}}
\newcommand \RR{\mathbb{R}}
\newcommand \NN{\mathbb{N}}

\newcommand{\dU}{\mathrm{Unif}}
\newcommand{\dUnif}{\mathrm{Unif}}

\DeclareMathOperator{\loss}{loss}

\newcommand{\hy}{\hat y}
\newcommand{\hb}{\hat\beta}

\usepackage{mathtools}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\scalp}{\langle}{\rangle}


\begin{document}

\section*{Home assignment 1}

Deadline: 2024-09-16, 21:00.

\begin{enumerate}
\item Each day Elon Musk solves econometrics problems and creates posts in X.
Let $y_i$ be the number of solved problems and $x_i$ be the number of posts in X. 
You have 3 observations: $x_1 = 2$, $y_1 = 5$, $x_2 = 1$, $y_2 = 10$, $x_3 = 3$, $y_3 = 4$.

\begin{enumerate}
    \item Find $\hb$ if fitted values are given by $y_i = \hb x_i$.
    \item Find $\hb_0$ and $\hb_1$ if fitted values are given by $y_i = \hb_0 + \hb_1 x_i$.
    \item Find $\hb_0$, $\hb_1$ and $\hb_2$ if fitted values are given by $y_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2$.
\end{enumerate}

Note: you can use any programming language to calculate the $3\times 3$ matrix inverse but you should provide the code :)

\item Simplify as much as possible the following expressions:
\[
A = \sum_{i=1}^n (x_i - \bar x)\bar x, \quad B = \sum_{i=1}^n (x_i - \bar x)\bar y, \quad C = \sum (x_i - \bar x)^2 + n \bar x^2.
\]

\item  Consider simple regression model with $\hat y_i = \hb_0 + \hb_1 x_i$.
You have $n$ observations $(x_1, y_1)$, \dots, $(x_n, y_n)$ and you estimate $\hb_0$ and $\hb_1$ using OLS. 

What will happen with $\hb_0$ and $\hb_1$ in each of the following cases?

\begin{enumerate}
    \item You copy every observation from the original dataset twice.
    \item You add one new observation $(y_{n+1} = \bar y, x_{n+1} = \bar x)$ to the original dataset.
    \item You add $n$ more observations given by $(x_{n+i} = -x_i, y_{n+i} = y_i)$ for $i = 1$, $2$, \dots, $n$ to the original dataset.
\end{enumerate}

Hint: you may start by guessing the answer with an experiment, but the proof is required :)

\end{enumerate}



\section*{Home assignment 2}

Deadline: 2024-09-23, 21:00.

\begin{enumerate}

\item Each day Elon Musk solves econometrics problems and creates posts in X.
Let $y_i$ be the number of solved problems and $x_i$ be the number of posts in X. 
You have 3 observations: $x_1 = 2$, $y_1 = 5$, $x_2 = 1$, $y_2 = 10$, $x_3 = 3$, $y_3 = 4$.

\begin{enumerate}
    \item Calculate $SST$, $SSE$, $SSR$ and $R^2$ if we regress $y$ on $x$ with constant, ie $\hy_i = \hb_0 + \hb_1 x_i$.
    \item Calculate $SST$, $SSE$, $SSR$ and $R^2$ if we regress $x$ on $y$ with constant, ie $\hat x_i = \hat\gamma_0 + \hat\gamma_1 y_i$.
    \item Calculate the hat-matrix $H$ if we regress $y$ on $x$ with constant.
\end{enumerate}


Note: this exercises uses toy dataset from the previous HA, you may reuse old results provided that you state them explicitely. 

\item Kamala Harris removes one observation 
from the initial set of $n$ observations and reestimates the model $\hy_i = \hb_0 + \hb_1 x_i$ using OLS.

\begin{enumerate}
    \item Prove that the total sum of squares $SST$ can't increase. 
    \item Provide an example of a dataset where explained sum of squares $SSE$ will decrease and a second example 
    where it will increase. 
\end{enumerate}
  
\item Consider the dataset of diamond prices,

\url{https://github.com/vincentarelbundock/Rdatasets/raw/master/csv/ggplot2/diamonds.csv}.

Here \verb|price| is the price of diamond in \$ and \verb|carat| is the weight of a diamond in carats.
Let $y_i$ be the log of diamond price in 1000\$ and $x_i$ be the log of diamond weight in carats. 

\begin{enumerate}
    \item Estimate the model $\hy_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2$ using \verb|LinearRegression| from \verb|sklearn.linear_model|.
    \item Estimate the model $\hy_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2$ using \verb|ols| from \verb|statsmodels.formula.api|.
    \item What is your point forecast of a price of a diamond with $2$ carats weight?
\end{enumerate}

Note: the first approach is faster and more stable while the second one gives you much more statistical information. 

\end{enumerate}


\section*{Home assignment 3}

Deadline: 2024-09-30, 21:00.

\begin{enumerate}

\item Consider the framework of simple regression model, $y_i = \beta_0 + \beta_1 x_i + u_i$, 
$\E(u_i \mid x) = 0$, independent observations, $\Var(u_i \mid x) = \sigma^2$, $\Cov(u_i, u_j \mid x) = 0$ for $i\neq j$.
We estimate regression $\hat y_i = \hb_0 + \hb_1 x_i$.

We have $n=3$ observations with $x_i = i$.

\begin{enumerate}
    \item Find $\E(2\hb_0 + 3\hb_1 \mid x)$, $\Var(2\hb_0 + 3\hb_1 \mid x)$.
    \item Find $\E(\hy_1 \mid x)$, $\Var(\hy_1 \mid x)$, $\E(\hat u_1 \mid x)$, $\Var(\hat u_1 \mid x)$.
\end{enumerate}

\item Consider the framework of simple regression model, $y_i = \beta_0 + \beta_1 x_i + u_i$, 
$\E(u_i \mid x) = 0$, independent observations, $\Var(u_i \mid x) = \sigma^2$, $\Cov(u_i, u_j \mid x) = 0$ for $i\neq j$.
We estimate regression $\hat y_i = \hb_0 + \hb_1 x_i$.

We have $n$ observations with $\sum (x_i - \bar x)^2 > 0$.

\begin{enumerate}
    \item Find $\E(y_i - \bar y \mid x)$, $\E((y_i - \bar y)^2 \mid x)$.
    \item It possible find the value of $\gamma$ such that the estimator $s^2 = \gamma \sum_{i=1}^n (y_i - \bar y)^2$ for $\sigma^2$ 
    is unbiased conditional on $x$. 
\end{enumerate}


\item Consider the framework of simple regression model, $y_i = \beta_0 + u_i$, $\beta_0 = 2$,
$\E(u_i \mid x) = 0$, independent observations, $\Var(u_i \mid x) = \sigma^2 = 4$, $\Cov(u_i, u_j \mid x) = 0$ for $i\neq j$.
Random error is conditionally normally distributed, $(u_i \mid x) \sim \cN(0; 4)$.
We estimate regression $\hat y_i = \hb_0 + \hb_1 x_i$.
This setup means that we wrongly belive that $y_i$ depends on $x_i$.

We have $n= 10$ observations with $x_i \sim \cN(0; 1)$.

\begin{enumerate}
    \item Generate the dataset and estimate the misspecified regression $B = 10000$ times. 
    Draw the histogram of $\hb_0$, the histogram of $\hb_1$. 
    Compare these histograms with true values of $\beta_0$ and $\beta_1$.
    What can you conclude based on two histogram?
    \item Draw the histogram of $R^2$ for simulations in point (a). 
    Now repeat $B = 10000$ simulations for regression $\hat y_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2 + \hb_3 x_i^3$.
    Draw the new histogram of $R^2$. 
    Describe how this new histogram for $R^2$ is different from the first histogram for $R^2$.
    Can you say that the quality of your new regression is higher?
\end{enumerate}

\end{enumerate}


\section*{Home assignment 4}

Deadline: 2024-10-07, 23:59.

\begin{enumerate}
\item Consider the simple regression model $y_i = \beta_0 + \beta_1 x_i + u_i$ with fitted values given by $\hat y_i = \hb_0 + \hb_1 x_i$ and $\hat u_i = y_i - \hat y_i$.
We have $n = 3$ observations, $x_i = i$, all Gauss-Markov assumptions are satisfied,
We use ordinary least squares.
\begin{enumerate}
    \item Write $\hb_1$ explicitely as a linear function of $(y_i)$, $\hb_1 = w_1 y_1 + w_2 y_2 + w_3 y_3$.
    \item Propose different coefficients $w_1'$, $w_2'$, $w_3'$ such that the estimator $\hb_1' =  w_1' y_1 + w_2' y_2 + w_3' y_3$ is unbiased for $\hb_1'$.
    \item Check that the variance of alternative estimator $\hb_1'$ is larger than the variance of OLS-estimator $\hb_1$.
    \item Find all the diagonal elements of the hat-matrix $H_{ii}$. 
    Which actual value $y_i$ has more influence on the forecasted value $\hat y_i$?
\end{enumerate}


\item Consider the multivariate regression model in a matrix form, $y = X\beta + u$ with fitted values given by $\hat y = X\hb$ and $\hat u = y - \hat y$.
We have $n$ observations, all Gauss-Markov assumptions are satisfied,
We use ordinary least squares.

\begin{enumerate}
    \item Find $\E(\hat u \mid X)$, $\E(\hat y \mid X)$.
    \item Find $\Var(\hat u \mid X)$, $\Cov(\hat y, \hb \mid X)$, $\Cov(\hat u, \hb \mid X)$.
\end{enumerate}

\item Consider the simple regression model $y_i = \beta_0 + \beta_1 x_i + u_i$ with fitted values given by $\hat y_i = \hb_0 + \hb_1 x_i$ and $\hat u_i = y_i - \hat y_i$.
We have $n$ observations, all Gauss-Markov assumptions are satisfied.
Yusuf Dikeç copies every observation twice and estimates regression using OLS for $2n$ observations. 

\begin{enumerate}
    \item Which Gauss-Markov assumptions are violated for the doubled dataset of $2n$ observations?
    \item Find the true conditional variance of $\hb_1$ in the regression on $2n$ observations assuming Gauss-Markov assumptions for the original dataset.
    \item Find the conditional variance of $\hb_1$ in the regression on $2n$ observations wrongly assuming Gauss-Markov assumptions for the doubled dataset.
\end{enumerate}

\end{enumerate}


\section*{Home assignment 5}

Deadline: No deadline. 

If you wish to upload something somewhere then you are free to submit econometrics memes to the chat. 


\section*{Home assignment 6}

Deadline: 2024-10-24 (updated 2024-10-21), 23:59.

\begin{enumerate}
    \item Consider the model $y_i = \beta_x x_i + \beta_w w_i + u_i$ with
    \[
    \begin{pmatrix}
        x_i \\
        w_i \\
        u_i \\
    \end{pmatrix} \sim \cN\left(
        \begin{pmatrix}
            0 \\
            0 \\
            0 \\
        \end{pmatrix};
    \begin{pmatrix}
        5 & 3 & 0 \\
        3 & 10 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \right).
    \]
    Observations are independent. 
    \begin{enumerate}
        \item Find the probability limit of $\hat \gamma_x$ in regression $\hat y_i = \hat \gamma_x x_i$.
        \item Is $\hat \gamma_x$ consistent estimator of $\beta_x$?
        \item Find the conditional expected value $\E(y_i \mid x_i)$.
        
        Hint: it should be of the form $\alpha x_i$, where $\alpha$ is a function of $\beta_x$ and $\beta_w$.
        \item Is $\hat \gamma_x$ consistent estimator of $\alpha$? 
    \end{enumerate}

    \item Consider the simple regression model $y_i = \beta_0 + \beta_1 x_i + u_i$.
    We do not observe $x_i$. Instead we observe two independent measurements of $x_i$: $x'_i$ and $x''_i$.
    Here $x'_i = x_i + v_i$ and $x''_i = x_i + w_i$, where $v_i$ and $w_i$ are measurement errors.

    Observations are independent, random variables $x_i$, $u_i$, $v_i$ and $w_i$ are independent. 
    Let's denote their variance by $\Var(x_i) = \sigma^2_x$, $\Var(u_i) = \sigma^2_u$, $\Var(v_i) = \sigma^2_v$, $\Var(w_i) = \sigma^2_w$.

    \begin{enumerate}
        \item Check whether the estimator $\hb_1^A$ is consistent for $\beta_1$:
        \[
        \hb_1^A = \frac{\sum (y_i - \bar y)(x'_i - \bar x')}{\sum (x'_i - \bar x')(x''_i - \bar x'')}.
        \]        
        \item Check whether the estimator $\hb_1^B$ is consistent for $\beta_1$:
        \[
            \hb_1^B = \frac{\sum (y_i - \bar y)(x'_i - \bar x')}{\sum (x''_i - \bar x'')^2}.
        \]
    \end{enumerate}
    
    \item Consider again the dataset of diamond prices,

    \url{https://github.com/vincentarelbundock/Rdatasets/raw/master/csv/ggplot2/diamonds.csv}.
    
    Here \verb|price| is the price of diamond in \$ and \verb|carat| is the weight of a diamond in carats.
    Let $y_i$ be the log of diamond price in 1000\$ and $x_i$ be the log of diamond weight in carats. 
    
   \begin{enumerate}
    \item Reestimate the model $\hy_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2$ using \verb|ols| from \verb|statsmodels.formula.api|.
   \end{enumerate}
   Let's believe in Gauss~— Markov assumptions for this case. 
   \begin{enumerate}[resume]
    \item Extract $SSRes$, $SST$, $SSExpl$,  $R^2$ and $\hat\sigma^2$.
    \item Write down true $\Var(\hat\beta \mid X)$ matrix. 
    
    Hint: your matrix should contain unknown $\sigma^2$. 
    \item Write down the estimate of $\Var(\hat\beta \mid X)$ matrix. 
    
    Hint: no unknown parameters here. 

    \item Calculate all diagonal entries $H_{ii}$ and select the most influential observation with highest $\partial \hat y_i /\partial y_i$.
    
    Hint: the whole matrix $H$ is really HUGE here, please do not try to calculate it, you need only diagonal elements. 

    \item Draw the scatterplot of $\abs{\hat u_i}$ against $x_i$. 
    
    Does this plot suggests that Gauss~— Markov assumptions are satisfied?
   \end{enumerate}

\end{enumerate}

\section*{Home assignment 7}

Deadline: 2024-11-02, 23:59.


% n = 200
% set.seed(777)
% u = rnorm(n, sd=2)
% x = rnorm(n, sd=3)
% w = rnorm(n, mean=x - 3, sd=2)
% h = x + w + rnorm(n, sd=4)
% y = 0.5 + 0.5*x + 0.5*w + 0.5*h + u
% X = cbind(1, x, w, h)
% XX = crossprod(X)
% Xy = crossprod(X, y)
% hb = solve(XX, Xy)
% hb
% yhat = X %*% hb
% uhat = y - yhat
% SSR = sum(uhat^2)
% hvar = SSR / (n - 4) * solve(XX)
% hvar


\begin{enumerate}
    \item Consider a simple regression model with Gauss~— Markov assumptions, $y_i = \beta_0 + \beta_1 x_i + u_i$.
    Random errors are jointly normal $u \mid X \sim \cN(0; \sigma^2 I)$.

    You know that 
    \[
    X^TX = \begin{pmatrix}
        100 & 200 \\
        ? & 600 \\
    \end{pmatrix}, \quad X^T y = \begin{pmatrix}
        0 \\
        300 \\
    \end{pmatrix}, \quad y^T y = 1000.
    \]
    \begin{enumerate}
        \item By looking at $X^TX$ recover the number of observations. 
        \item Estimate $\hb_0$, $\hb_1$, $\hVar(\hb \mid X)$.
        \item Test $H_0: \beta_1 = 0$ against $H_1: \beta_1 \neq 0$ at significance level $\alpha = 0.05$.
        \item Construct 99\% confidence interval for $\beta_0$ and $\beta_1$.
        \item Esimate $\E(y_{101} \mid x_{101} = 5)$ and construct 99\% confidence interval for $\E(y_{101} \mid x_{101} = 5)$.
    \end{enumerate}

    \item You estimated the vector $\beta = (\beta_0, \beta_1, \beta_2, \beta_3)^T$ in the model 
    \[
    y_i = \beta_0 + \beta_1 x_i + \beta_2 w_i + \beta_3 h_i + u_i
    \]
    using OLS with $200$ observations. 
    Gauss~— Markov assumptions are satisfied. 
    Random errors are jointly normal $u \mid X \sim \cN(0; \sigma^2 I)$.

    You know that 
    \[
    \hb = \begin{pmatrix}
        0.2 \\
        0.5 \\
        0.3 \\
        0.6 \\
    \end{pmatrix}, \quad 
    \hVar(\hb \mid X) = 
    0.001 \cdot \begin{pmatrix}
        63.14 & -14.78 & 15.56 & 0.335 \\
        ?  & 7.912 & -3.943 & -1.065 \\
        ?  &   ?  &   6.939 & -1.375 \\
        ?  &   ?  &    ?  & 1.178 \\
    \end{pmatrix}.
    \]
    \begin{enumerate}
        \item Test $H_0$: $\beta_1 = \beta_2$ using significance level $\alpha = 0.05$ against $H_1$: $\beta_1 \neq \beta_2$.
        \item Test $H_0$: $\beta_1 + \beta_2 = 1$ using significance level $\alpha = 0.05$ against $H_1$: $\beta_1 + \beta_2 \neq 1$.
        \item Construct 99\% confidence interval for $\beta_1 + 2\beta_2 + 3\beta_3$.
    \end{enumerate}


    \item Researches suspect that «College students often have poor sleep habits, 
    staying up late and sleeping short hours, 
    and a great deal of research suggests that lack of sleep can harm cognitive performance». 

    Let's build a model where dependent variable is \verb|term_gpa| and other variables below as predictors:
    \begin{itemize}
    \item \verb|demo_race|: binary label for underrepresented and non-underrepresented students; 
    
    \item \verb|demo_gender|: Gender of the subject (male = 0, female = 1);
    \item \verb|bedtime_mssd|: Mean successive squared difference of bedtime;
    
    \item \verb|TotalSleepTime|: Average time in bed in minutes;
    
    \item \verb|cum_gpa|: Cumulative GPA (out of 4.0), for semesters before the one being studied;
    
    \item \verb|term_gpa|: End-of-term GPA (out of 4.0) for the semester being studied;
    
    \item \verb|units_score|: Standardized number of course units carried in the term;
    \end{itemize}
    
    More info can be found at: 
    
    \url{https://cmustatistics.github.io/data-repository/psychology/cmu-sleep.html}.
    
    \begin{enumerate}
        \item Check that the dataset is imported correctly! Remove missing observations. 
        \item Estimate the model using OLS.
        \item Test the hypothesis that the effect of \verb|TotalSleepTime| is zero.
        \item Test the hypothesis that the sum of effects for \verb|demo_gender|, \verb|bedtime_mssd| and \verb|units_score| is nonzero.
        \item Test the hypothesis that additional two hours of sleep every day gives additional 0.3 gpa point on average.
        \item Test the hypothesis that the parameters \verb|demo_gender|, \verb|bedtime_mssd| and \verb|units_score| are jointly insignificant.
    \end{enumerate}
    

\end{enumerate}




\section*{Home assignment 8}

Deadline: 2024-11-27, 23:59.


\begin{enumerate}
\item Let $A$, $B$, $a$, $b$ — be matrices and vectors of constants and $V$ and $v$ — matrix and vector that depends on some arguments. 

\begin{enumerate}
    \item Find $d(AVB + v^Tb + a^T v)$.
    \item By applying differential $d$ to the identity $V \cdot V^{-1} = I$ find $d(V^{-1})$.
    \item Assuming that $A$ is symmetric find $d(v'Av)$.
    \item Assuming that $A$ is symmetric find $d(v'Av / v'v)$.
\end{enumerate}

\item Consider the simple regression with $L^1$-penalty with three observations, $y = (1, 3, 5)$, $x = (1, 2, 2)$.
The loss function to be minimized is given by 
\[
\loss(\hb_0, \hb_1) = \sum_i (y_i - \hat y_i)^2 + \lambda \cdot \abs{\hb_1}, \quad \hat y_i = \hb_0 + \hb_1 x_i.
\] 

\begin{enumerate}
    \item Find optimal coefficients for $\lambda = 0$. 
    \item Find optimal coefficients for $\lambda = +\infty$.
    \item Find optimal coefficients for $\lambda = 1$.
    \item Find optimal coefficients for any $\lambda \geq 0$.
\end{enumerate}

\item Consider the following experiment. 
Generate $n=200$ observations under for the model $y_i = -2 + 7 x_i + 2 w_i + u_i$,
where $w_i = x_i^2 + v_i$.
Random variables $x_i$, $v_i$ and $u_i$ are independent, $x_i \sim \cN(1; 1)$, $v_i \sim \cN(1; 1)$, $u_i \sim \dUnif[-5; 5]$. 

Simulate the experiment once.
\begin{enumerate}
    \item Estimate coefficients using OLS.
    \item Find 95\% classic confidence interval for $\beta_w$.
    \item Find 95\% pair bootstrap confidence interval for $\beta_w$.
    \item Find 95\% t-statistic bootstrap confidence interval for $\beta_w$.
    \item Find 95\% pair bootstrap confidence interval for $\beta_x - \beta_w^3$.
    \item Test hypothesis $H_0$: $\beta_x = \beta_w^3$ at 5\% significance level against  $H_1$: $\beta_x \neq \beta_w^3$.
\end{enumerate}

Now simulate the experiment 5000 times. 
\begin{enumerate}[resume]
    \item For each simulation do points (a-e).
    \item Estimate the real coverage probability of classic confidence interval for $\beta_w$. 
    
    Hint: Just count the number of simulation where true beta belongs to the interval :)
    \item Estimate the real coverage probability of pair bootstrap confidence interval for $\beta_w$. 
    \item Estimate the real coverage probability of t-statistic bootstrap confidence interval for $\beta_w$.
    \item Estimate the real coverage probability of pair bootstrap confidence interval for $\beta_x - \beta_w^3$. 
\end{enumerate}


\end{enumerate}


\section*{Home assignment 9}

Deadline: 2024-12-15, 23:59.

\begin{enumerate}
    \item Consider the model $y_i = \beta_1 + \beta_x x_i + u_i$ where $\E(u_i \mid x) = 0$, but $\Var(u_i \mid x) = \sigma^2 \cdot x^2_i$.
    We have a toy dataset $x = (1, 2, -1)$ and $y = (3, 8, 8)$.
    
    \begin{enumerate}
        \item Find ordinary least squares estimates $\hb_{\text{ols}}$.
        \item Find weighted least squares estimates $\hb_{\text{wls}}$ with minimal variance in this case. 
        \item Find the true covariance matrix $\Var(\hb_{\text{ols}} \mid x)$.
        \item Find the true covariance matrix $\Var(\hb_{\text{wls}} \mid x)$.
        \item Find the hypothetical covariance matrix $\Var(\hb_{\text{ols}} \mid x, H_0)$ assuming homoskedasticity in $H_0$.
        \end{enumerate}
        

    \item The variables $a$, $b$ and $c$ are standardized. 
    The sample correlation of $a$ with $b$ and $c$ is zero. 
    The sample correlation of $b$ and $c$ is $0.5$.

    \begin{enumerate}
        \item Write the correlation matrix $C$ of these variables and find the SVD of $C$. 
        \item Find two matrices from the SVD of the matrix $X$ with columns $a$, $b$ and $c$.
        \item Which proportion of the total variance is explained by the first component?
        \item Which proportion of the total variance is explained by the first two components?
        \item Express the first two components in terms of the original predictors, $a$, $b$ and $c$. 
        \item We have estimated regression of some centered variable $y$ onto the first two components,
        \[
        \hat y_i = 0.9 p_{i1} - 0.3 p_{i2}.
        \]
        Provide an approximate reconstruction of the coefficients $\hb_a$, $\hb_b$, $\hb_c$ 
        in the regression 
        \[
        \hat y_i = \hb_a a_i + \hb_b b_i + \hb_c c_i. 
        \]
    \end{enumerate}

    \item Hakuna matata :)

\end{enumerate}

\end{document}


% проектируемая часть ниже ....