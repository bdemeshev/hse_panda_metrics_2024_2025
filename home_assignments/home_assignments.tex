% arara: xelatex
\documentclass[12pt]{article}

% \usepackage{physics}


\usepackage{tikzducks}

\usepackage{tikz} % картинки в tikz
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering}

\usepackage{amsmath, amsfonts, amssymb} % куча стандартных математических плюшек

\usepackage{comment}

\usepackage[top=2cm, left=1.2cm, right=1.2cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption}

\usepackage{url} % to use \url{link to web}


\newcommand{\smallduck}{\begin{tikzpicture}[scale=0.3]
    \duck[
        cape=black,
        hat=black,
        mask=black
    ]
    \end{tikzpicture}}

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{Home assignments for samurai}
\lfoot{}
\cfoot{}
\rfoot{}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\usepackage{tcolorbox} % рамочки!

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos - печатает все поставленные \todo'шки


% более красивые таблицы
\usepackage{booktabs}
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"


\setcounter{MaxMatrixCols}{20}
% by crazy default pmatrix supports only 10 cols :)


\usepackage{fontspec}
\usepackage{libertine}
\usepackage{polyglossia}

\setmainlanguage{russian}
\setotherlanguages{english}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
% \setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
% \newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
% \setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*}

%% эконометрические сокращения
\DeclareMathOperator{\Cov}{\mathbb{C}ov}
\DeclareMathOperator{\Corr}{\mathbb{C}orr}
\DeclareMathOperator{\Var}{\mathbb{V}ar}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\row}{row}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}

\DeclareMathOperator{\E}{\mathbb{E}}
% \DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\card}{card}

\DeclareMathOperator{\Convex}{Convex}

\newcommand \cN{\mathcal{N}}
\newcommand \RR{\mathbb{R}}
\newcommand \NN{\mathbb{N}}


\newcommand{\hy}{\hat y}
\newcommand{\hb}{\hat\beta}



\begin{document}

\section*{Home assignment 1}

Deadline: 2024-09-16, 21:00.

\begin{enumerate}
\item Each day Elon Musk solves econometrics problems and creates posts in X.
Let $y_i$ be the number of solved problems and $x_i$ be the number of posts in X. 
You have 3 observations: $x_1 = 2$, $y_1 = 5$, $x_2 = 1$, $y_2 = 10$, $x_3 = 3$, $y_3 = 4$.

\begin{enumerate}
    \item Find $\hb$ if fitted values are given by $y_i = \hb x_i$.
    \item Find $\hb_0$ and $\hb_1$ if fitted values are given by $y_i = \hb_0 + \hb_1 x_i$.
    \item Find $\hb_0$, $\hb_1$ and $\hb_2$ if fitted values are given by $y_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2$.
\end{enumerate}

Note: you can use any programming language to calculate the $3\times 3$ matrix inverse but you should provide the code :)

\item Simplify as much as possible the following expressions:
\[
A = \sum_{i=1}^n (x_i - \bar x)\bar x, \quad B = \sum_{i=1}^n (x_i - \bar x)\bar y, \quad C = \sum (x_i - \bar x)^2 + n \bar x^2.
\]

\item  Consider simple regression model with $\hat y_i = \hb_0 + \hb_1 x_i$.
You have $n$ observations $(x_1, y_1)$, \dots, $(x_n, y_n)$ and you estimate $\hb_0$ and $\hb_1$ using OLS. 

What will happen with $\hb_0$ and $\hb_1$ in each of the following cases?

\begin{enumerate}
    \item You copy every observation from the original dataset twice.
    \item You add one new observation $(y_{n+1} = \bar y, x_{n+1} = \bar x)$ to the original dataset.
    \item You add $n$ more observations given by $(x_{n+i} = -x_i, y_{n+i} = y_i)$ for $i = 1$, $2$, \dots, $n$ to the original dataset.
\end{enumerate}

Hint: you may start by guessing the answer with an experiment, but the proof is required :)

\end{enumerate}



\section*{Home assignment 2}

Deadline: 2024-09-23, 21:00.

\begin{enumerate}

\item Each day Elon Musk solves econometrics problems and creates posts in X.
Let $y_i$ be the number of solved problems and $x_i$ be the number of posts in X. 
You have 3 observations: $x_1 = 2$, $y_1 = 5$, $x_2 = 1$, $y_2 = 10$, $x_3 = 3$, $y_3 = 4$.

\begin{enumerate}
    \item Calculate $SST$, $SSE$, $SSR$ and $R^2$ if we regress $y$ on $x$ with constant, ie $\hy_i = \hb_0 + \hb_1 x_i$.
    \item Calculate $SST$, $SSE$, $SSR$ and $R^2$ if we regress $x$ on $y$ with constant, ie $\hat x_i = \hat\gamma_0 + \hat\gamma_1 y_i$.
    \item Calculate the hat-matrix $H$ if we regress $y$ on $x$ with constant.
\end{enumerate}


Note: this exercises uses toy dataset from the previous HA, you may reuse old results provided that you state them explicitely. 

\item Kamala Harris removes one observation 
from the initial set of $n$ observations and reestimates the model $\hy_i = \hb_0 + \hb_1 x_i$ using OLS.

\begin{enumerate}
    \item Prove that the total sum of squares $SST$ can't increase. 
    \item Provide an example of a dataset where explained sum of squares $SSE$ will decrease and a second example 
    where it will increase. 
\end{enumerate}
  
\item Consider the dataset of diamond prices,

\url{https://github.com/vincentarelbundock/Rdatasets/raw/master/csv/ggplot2/diamonds.csv}.

Here \verb|price| is the price of diamond in 1000\$ and \verb|carat| is the weight of a diamond in carats.
Let $y_i$ be the log of diamond price in 1000\$ and $x_i$ be the log of diamond weight in carats. 

\begin{enumerate}
    \item Estimate the model $\hy_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2$ using \verb|LinearRegression| from \verb|sklearn.linear_model|.
    \item Estimate the model $\hy_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2$ using \verb|ols| from \verb|statsmodels.formula.api|.
    \item What is your point forecast of a price of a diamond with $2$ carats weight?
\end{enumerate}

Note: the first approach is faster and more stable while the second one gives you much more statistical information. 

\end{enumerate}


\section*{Home assignment 3}

Deadline: 2024-09-30, 21:00.

\begin{enumerate}

\item Consider the framework of simple regression model, $y_i = \beta_0 + \beta_1 x_i + u_i$, 
$\E(u_i \mid x) = 0$, independent observations, $\Var(u_i \mid x) = \sigma^2$, $\Cov(u_i, u_j \mid x) = 0$ for $i\neq j$.
We estimate regression $\hat y_i = \hb_0 + \hb_1 x_i$.

We have $n=3$ observations with $x_i = i$.

\begin{enumerate}
    \item Find $\E(2\hb_0 + 3\hb_1 \mid x)$, $\Var(2\hb_0 + 3\hb_1 \mid x)$.
    \item Find $\E(\hy_1 \mid x)$, $\Var(\hy_1 \mid x)$, $\E(\hat u_1 \mid x)$, $\Var(\hat u_1 \mid x)$.
\end{enumerate}

\item Consider the framework of simple regression model, $y_i = \beta_0 + \beta_1 x_i + u_i$, 
$\E(u_i \mid x) = 0$, independent observations, $\Var(u_i \mid x) = \sigma^2$, $\Cov(u_i, u_j \mid x) = 0$ for $i\neq j$.
We estimate regression $\hat y_i = \hb_0 + \hb_1 x_i$.

We have $n$ observations with $\sum (x_i - \bar x)^2 > 0$.

\begin{enumerate}
    \item Find $\E(y_i - \bar y \mid x)$, $\E((y_i - \bar y)^2 \mid x)$.
    \item Find the value of $\gamma$ such that the estimator $s^2 = \gamma \sum_{i=1}^n (y_i - \bar y)^2$ for $\sigma^2$ 
    is unbiased conditional on $x$. 
\end{enumerate}


\item Consider the framework of simple regression model, $y_i = \beta_0 + u_i$, $\beta_0 = 2$,
$\E(u_i \mid x) = 0$, independent observations, $\Var(u_i \mid x) = \sigma^2 = 4$, $\Cov(u_i, u_j \mid x) = 0$ for $i\neq j$.
Random error is conditionally normally distributed, $(u_i \mid x) \sim \cN(0; 4)$.
We estimate regression $\hat y_i = \hb_0 + \hb_1 x_i$.
This setup means that we wrongly belive that $y_i$ depends on $x_i$.

We have $n= 10$ observations with $x_i \sim \cN(0; 1)$.

\begin{enumerate}
    \item Generate the dataset and estimate the misspecified regression $B = 10000$. 
    Draw the histogram of $\hb_0$, the histogram of $\hb_1$. 
    Compare these histograms with true values of $\beta_0$ and $\beta_1$.
    What can you conclude based on two histogram?
    \item Draw the histogram of $R^2$ for simulations in point (a). 
    Now repeat $B = 10000$ simulations for regression $\hat y_i = \hb_0 + \hb_1 x_i + \hb_2 x_i^2 + \hb_3 x_i^3$.
    Draw the new histogram of $R^2$. 
    Describe how this new histogram for $R^2$ is different from the first histogram for $R^2$.
    Can you say that the quality of your new regression is higher?
\end{enumerate}

\end{enumerate}


\end{document}

% здесь проектируемая часть

\end{document}

